{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f409967-209c-4114-9cf0-82ac32abff25",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0dfdd63a-b386-4420-9b06-391cdd5828b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pathlib\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "%matplotlib qt5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4bad0b11-869d-4285-b7b1-30de699784d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rtseg.cellseg.dataloaders import PhaseContrast\n",
    "from rtseg.cellseg.utils.transforms import transforms\n",
    "from rtseg.cellseg.networks import model_dict\n",
    "from torch.utils.data import DataLoader\n",
    "from rtseg.cellseg.utils.tiling import get_tiler\n",
    "from rtseg.cellseg.numerics.vf_to_masks import construct_mask, construct_masks_batch\n",
    "from skimage.io import imread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76a61145-3344-4b69-af3a-0ac0cd02f0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "device='cuda:0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "035e588f-bb59-482b-80c8-9b5f884379c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = Path('/home/pk/Documents/rtseg/models/cellseg/checkpoints/2024-07-22_14-44-13/model_val.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "48b98538-b071-4729-8be3-e7d74c5197ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model_dict['ResUnet']\n",
    "model = model.parse(channels_by_scale=[1, 32, 64, 128, 256], num_outputs=[1, 2, 1],\n",
    "                    upsample_type='transpose_conv', feature_fusion_type='concat',\n",
    "                    skip_channel_seg=True)\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "tiler = get_tiler(\"dynamic_overlap\")\n",
    "wrapped_model = tiler(model, device=device)\n",
    "wrapped_model = wrapped_model.to(device)\n",
    "\n",
    "def run_segnet(image):\n",
    "    with torch.inference_mode():\n",
    "        pred_semantic, pred_vf = wrapped_model(image)\n",
    "    return pred_semantic, pred_vf\n",
    "\n",
    "# plot the outputs of one image\n",
    "def plot_inference(pred_semantic, pred_vf, device='cpu'):\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=3)\n",
    "    ax[0].imshow(pred_semantic[0][0].cpu().numpy())\n",
    "    ax[0].set_title(\"Semantic\")\n",
    "    ax[1].imshow(pred_vf[0][0].cpu().numpy())\n",
    "    ax[1].set_title(\"vf_x\")\n",
    "    ax[2].imshow(pred_vf[0][1].cpu().numpy())\n",
    "    ax[2].set_title(\"vf_y\")\n",
    "    plt.show()\n",
    "\n",
    "def tensorize_image(image, device='cuda:0'):\n",
    "    image_tensor = torch.from_numpy(image).float() / 65535.0\n",
    "    image_tensor = image_tensor[None, None, :].to(device) # add dimension to play well with (N, C, H, W)\n",
    "    return image_tensor\n",
    "    \n",
    "def segment(image, device='cuda:0'):\n",
    "    pred_semantic, pred_vf = run_segnet(tensorize_image(image, device=device))\n",
    "    segmentation_mask = construct_masks_batch(pred_vf, pred_semantic, device=device, store_solutions=False, fast=True)\n",
    "    return segmentation_mask[0][0].cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dc5a647d-910b-48e9-8773-b78e248b5b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "phase_img = imread('/home/pk/Documents/rtseg/data/timelapse/_1/Default/img_channel000_position000_time000000000_z000.tif')\n",
    "#phase_img = imread('/mnt/sda1/REALTIME/data/seg_unet/dual/phase/img0049.tif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b17228-444f-4179-87ea-eaee8d38a424",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2fdd667b-d33f-40cf-b1a7-ef70cb8ad301",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.imshow(phase_img, cmap='gray')\n",
    "plt.title(f\"{phase_img.shape}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3eeb101d-f500-42a4-abd8-fd80968eaef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = segment(phase_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "de0f5738-c292-444e-aaf0-ce449e736519",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.imshow(mask)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d1015c45-82db-45a8-918d-fdf0e491a647",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rtseg.cellseg.utils.transforms import train_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "83f359fb-3268-44c8-b493-9344df5b9c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = Path('/mnt/sda1/REALTIME/data/seg_unet/dual/')\n",
    "train_transforms = train_transform\n",
    "\n",
    "train_ds = PhaseContrast(phase_dir=train_dir/Path('phase'),\n",
    "                labels_dir=train_dir/Path('mask'),\n",
    "                vf_dir=train_dir/Path('vf11'),\n",
    "                vf_at_runtime=True, # Vf are computed on the fly\n",
    "                labels_delimiter='',\n",
    "                vf_delimiter='_vf_11',\n",
    "                transforms=train_transform,\n",
    "                phase_format='.tif',\n",
    "                labels_format='.tif',\n",
    "                vf_format='.npy'\n",
    "            )\n",
    "train_dl = DataLoader(train_ds, batch_size=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b82293a6-d4a4-4293-9a10-5190c1781068",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<rtseg.cellseg.utils.transforms.Compose at 0x7b60841081d0>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds.transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "47882c5d-5f7a-4bff-bf88-c93d4204601d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = iter(train_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bd70038e-8b90-4d23-8561-11704246df0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = next(train_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4109828b-c432-42f3-910f-6236bbdcaa82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([6, 1, 320, 320]),\n",
       " torch.Size([6, 1, 320, 320]),\n",
       " torch.Size([6, 2, 320, 320]))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0].shape, a[1].shape, a[2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bda638b-1281-48b9-89b9-1c8d1536beed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5c7c11-8e50-40eb-9bb6-ef6310a7f380",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "537aba58-3db9-4872-b2de-62cfa441c962",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rtseg.cellseg.utils.transforms import train_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "725e8ca8-37e8-4ca2-8b3b-6a20ddd0b661",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Compose.__call__() takes 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m t \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_ds\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "File \u001b[0;32m~/Documents/rtseg/rtseg/cellseg/dataloaders.py:114\u001b[0m, in \u001b[0;36mPhaseContrast.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/rtseg/rtseg/cellseg/dataloaders.py:97\u001b[0m, in \u001b[0;36mPhaseContrast._get_image_mask_vf\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;66;03m#vf = np.load(vf_filename).astype(np.float32)\u001b[39;00m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 97\u001b[0m     image, mask, vf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransforms\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m image, mask, vf\n",
      "\u001b[0;31mTypeError\u001b[0m: Compose.__call__() takes 2 positional arguments but 3 were given"
     ]
    }
   ],
   "source": [
    "t = train_ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "725dd5c1-f8bb-4863-aca4-b2d67d69afd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((880, 1024), (880, 1024))"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t[0].shape, t[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "a0275cda-3969-4ba7-b974-9806f53fc88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=2)\n",
    "ax[0].imshow(t[0], cmap='gray')\n",
    "ax[1].imshow(t[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "d624329f-ef67-44d7-bcc5-a51c308d5960",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms.functional  as TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "a13c2a7b-0143-431c-8662-d95f5fb2630b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<rtseg.cellseg.utils.transforms.changedToPIL object at 0x7e849e41d010>\n",
      "<rtseg.cellseg.utils.transforms.RandomCrop object at 0x7e849e41d390>\n",
      "<rtseg.cellseg.utils.transforms.RandomRotation object at 0x7e849e69c690>\n",
      "10.957586288452148\n",
      "<rtseg.cellseg.utils.transforms.RandomAffine object at 0x7e849e6360d0>\n",
      "<rtseg.cellseg.utils.transforms.VerticalFlip object at 0x7e849e668dd0>\n",
      "<rtseg.cellseg.utils.transforms.HorizontalFlip object at 0x7e849e3ffad0>\n",
      "<rtseg.cellseg.utils.transforms.AddVectorField object at 0x7e849e60e290>\n",
      "<rtseg.cellseg.utils.transforms.ToFloat object at 0x7e849e68ed90>\n"
     ]
    }
   ],
   "source": [
    "t1 = train_transform(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "5b3ba32d-d630-4ed4-bcba-a02a60dbb825",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 320, 320]),\n",
       " torch.Size([1, 320, 320]),\n",
       " torch.Size([2, 320, 320]))"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1[0].shape, t1[1].shape, t1[2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "b768980e-a07b-43be-ad7e-d32f360b273e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.imshow(t1[2][0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "f0f4687d-804d-45eb-a61d-ee6e35c024aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.measure import label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4823befa-eb8c-4f7a-9bc4-b1c180fe0b70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1567ddde-d5c7-4bce-a456-84c486615005",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rtseg.cellseg.numerics.sdf_vf import sdf_vector_field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bc8bf286-bda7-48e4-94b0-2f48638594f1",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Could not infer dtype of Image",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m a \u001b[38;5;241m=\u001b[39m sdf_vector_field(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt1\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;241m21\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Could not infer dtype of Image"
     ]
    }
   ],
   "source": [
    "a = sdf_vector_field(torch.tensor(t1[1]), 21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cef279bb-e5de-4393-93ec-b90d87fe6370",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 880, 1024])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "21cfae1b-c240-46e9-a465-621dd167248d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=2)\n",
    "ax[0].imshow(a[0].numpy())\n",
    "ax[1].imshow(a[1].numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dbb0cb6-f7b7-456f-ad89-7e2812f4e32a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c828d88d-d5fb-4d5b-a29b-402de6094f2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d1243f-d051-4f49-a18c-6ac9958d4158",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4f591b-20fe-44e6-a19e-867d4b8d410f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomRotateScale:\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def __call__(self, image, mask , vf = None):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5383bcb7-d85f-4bb4-b36e-d1faaa91566d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21f7e06-9113-414b-8175-e8c093b0c1de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a046ae-a01b-4259-a159-8bd32e9e0355",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a08827-9957-412e-9211-6b7583d121dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed54e7f0-c206-4807-b518-4c1bc6fa82f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743304fb-4508-4162-aee3-9dcd7c06a95c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cdaa3b8e-fee1-4427-ae10-aba38aea8309",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dir = Path('/mnt/sda1/REALTIME/data/seg_unet/dual/')\n",
    "test_transforms = transforms['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0b26fbec-438b-4684-a81f-70568a94066d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = PhaseContrast(phase_dir=test_dir/Path('phase'),\n",
    "                labels_dir=test_dir/Path('mask'),\n",
    "                vf_dir=test_dir/Path('vf11'),\n",
    "                vf=False,\n",
    "                labels_delimiter='',\n",
    "                vf_delimiter='_vf_11',\n",
    "                transforms=test_transforms,\n",
    "                phase_format='.tif',\n",
    "                labels_format='.tif',\n",
    "                vf_format='.npy'\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fee3002-895f-4524-a245-635681dd4ec3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0389dc76-6729-4362-b28e-b105399a7fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dl = DataLoader(test_ds, batch_size=1, pin_memory=False, drop_last=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7ac54b1c-bb59-44f4-937e-6f9b4a98cdc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_iter = iter(test_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "836b35ba-bdb4-4bd3-bcb5-a130fb6ca308",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Caught ValueError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/pk/Documents/rtseg/.venv/lib/python3.11/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/home/pk/Documents/rtseg/.venv/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/pk/Documents/rtseg/.venv/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n            ~~~~~~~~~~~~^^^^^\n  File \"/home/pk/Documents/rtseg/rtseg/cellseg/dataloaders.py\", line 110, in __getitem__\n    return self._getitem(idx)\n           ^^^^^^^^^^^^^^^^^^\n  File \"/home/pk/Documents/rtseg/rtseg/cellseg/dataloaders.py\", line 105, in _get_image_mask\n    image, mask = self.transforms(image, mask)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/pk/Documents/rtseg/rtseg/cellseg/utils/transforms.py\", line 55, in __call__\n    image, mask = layer(image, mask)\n    ^^^^^^^^^^^\nValueError: too many values to unpack (expected 2)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[49], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m phase, mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtest_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(phase\u001b[38;5;241m.\u001b[39mshape, mask\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[0;32m~/Documents/rtseg/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/Documents/rtseg/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1326\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_task_info[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rcvd_idx]) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m   1325\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_task_info\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rcvd_idx)[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m-> 1326\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1328\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   1329\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_data()\n",
      "File \u001b[0;32m~/Documents/rtseg/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1372\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1370\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_put_index()\n\u001b[1;32m   1371\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[0;32m-> 1372\u001b[0m     \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1373\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/Documents/rtseg/.venv/lib/python3.11/site-packages/torch/_utils.py:705\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    701\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    702\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    703\u001b[0m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 705\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[0;31mValueError\u001b[0m: Caught ValueError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/pk/Documents/rtseg/.venv/lib/python3.11/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/home/pk/Documents/rtseg/.venv/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/pk/Documents/rtseg/.venv/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n            ~~~~~~~~~~~~^^^^^\n  File \"/home/pk/Documents/rtseg/rtseg/cellseg/dataloaders.py\", line 110, in __getitem__\n    return self._getitem(idx)\n           ^^^^^^^^^^^^^^^^^^\n  File \"/home/pk/Documents/rtseg/rtseg/cellseg/dataloaders.py\", line 105, in _get_image_mask\n    image, mask = self.transforms(image, mask)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/pk/Documents/rtseg/rtseg/cellseg/utils/transforms.py\", line 55, in __call__\n    image, mask = layer(image, mask)\n    ^^^^^^^^^^^\nValueError: too many values to unpack (expected 2)\n"
     ]
    }
   ],
   "source": [
    "phase, mask = next(test_iter)\n",
    "\n",
    "print(phase.shape, mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "717671a4-352a-42ff-a2fa-90c61c05204a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 320, 320])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ded657be-ef87-4a48-8ead-4725d0ffb932",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(phase, mask):\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=2)\n",
    "    ax[0].imshow(phase[0][0].numpy(), cmap='gray')\n",
    "    ax[1].imshow(mask[0][0].numpy())\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6dfdca07-7909-4924-b2e3-de139c534de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(phase, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9088adf8-b745-4a6b-a372-7b0189e7feb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms.functional as TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b73e0c29-775b-4d07-9a94-0e3d0ba48565",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phase.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "039d4ae6-83ab-4ee2-b910-176f3e67e090",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.util import random_noise\n",
    "from skimage.exposure import adjust_gamma, rescale_intensity\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031d82fa-6b5f-4766-9155-49edf7ca6c66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b4a5365c-6927-46cc-9705-414c75ccc976",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8210256985135094\n",
      "1467.453322170123\n"
     ]
    }
   ],
   "source": [
    "phase_np = phase.numpy()\n",
    "gamma_factor = random.uniform(0.7, 1.4)\n",
    "print(gamma_factor)\n",
    "brightness_fator = random.uniform(-2500.0, 5000.0)\n",
    "print(brightness_fator)\n",
    "phase_adjusted = adjust_gamma(phase_np, gamma=gamma_factor)\n",
    "phase_adjusted += brightness_fator\n",
    "phase_adjust = rescale_intensity(phase_adjusted, in_range='image', out_range='uint16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e205350a-51ba-46bc-969e-3bfae985e37b",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Invalid shape (1, 320, 320) for image data",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[62], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m fig, ax \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots(nrows\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, ncols\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m im \u001b[38;5;241m=\u001b[39m \u001b[43max\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mphase_np\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m fig\u001b[38;5;241m.\u001b[39mcolorbar(im, ax\u001b[38;5;241m=\u001b[39max[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m      4\u001b[0m im \u001b[38;5;241m=\u001b[39m ax[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mimshow(phase_adjust[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[0;32m~/Documents/rtseg/.venv/lib/python3.11/site-packages/matplotlib/__init__.py:1465\u001b[0m, in \u001b[0;36m_preprocess_data.<locals>.inner\u001b[0;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1462\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m   1463\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(ax, \u001b[38;5;241m*\u001b[39margs, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1464\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1465\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43max\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msanitize_sequence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1467\u001b[0m     bound \u001b[38;5;241m=\u001b[39m new_sig\u001b[38;5;241m.\u001b[39mbind(ax, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1468\u001b[0m     auto_label \u001b[38;5;241m=\u001b[39m (bound\u001b[38;5;241m.\u001b[39marguments\u001b[38;5;241m.\u001b[39mget(label_namer)\n\u001b[1;32m   1469\u001b[0m                   \u001b[38;5;129;01mor\u001b[39;00m bound\u001b[38;5;241m.\u001b[39mkwargs\u001b[38;5;241m.\u001b[39mget(label_namer))\n",
      "File \u001b[0;32m~/Documents/rtseg/.venv/lib/python3.11/site-packages/matplotlib/axes/_axes.py:5759\u001b[0m, in \u001b[0;36mAxes.imshow\u001b[0;34m(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, interpolation_stage, filternorm, filterrad, resample, url, **kwargs)\u001b[0m\n\u001b[1;32m   5756\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m aspect \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   5757\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_aspect(aspect)\n\u001b[0;32m-> 5759\u001b[0m \u001b[43mim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5760\u001b[0m im\u001b[38;5;241m.\u001b[39mset_alpha(alpha)\n\u001b[1;32m   5761\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m im\u001b[38;5;241m.\u001b[39mget_clip_path() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   5762\u001b[0m     \u001b[38;5;66;03m# image does not already have clipping set, clip to axes patch\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/rtseg/.venv/lib/python3.11/site-packages/matplotlib/image.py:723\u001b[0m, in \u001b[0;36m_ImageBase.set_data\u001b[0;34m(self, A)\u001b[0m\n\u001b[1;32m    721\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(A, PIL\u001b[38;5;241m.\u001b[39mImage\u001b[38;5;241m.\u001b[39mImage):\n\u001b[1;32m    722\u001b[0m     A \u001b[38;5;241m=\u001b[39m pil_to_array(A)  \u001b[38;5;66;03m# Needed e.g. to apply png palette.\u001b[39;00m\n\u001b[0;32m--> 723\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_A \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_normalize_image_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    724\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_imcache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    725\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstale \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/rtseg/.venv/lib/python3.11/site-packages/matplotlib/image.py:693\u001b[0m, in \u001b[0;36m_ImageBase._normalize_image_array\u001b[0;34m(A)\u001b[0m\n\u001b[1;32m    691\u001b[0m     A \u001b[38;5;241m=\u001b[39m A\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# If just (M, N, 1), assume scalar and apply colormap.\u001b[39;00m\n\u001b[1;32m    692\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (A\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m A\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m A\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m4\u001b[39m]):\n\u001b[0;32m--> 693\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mA\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for image data\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    694\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m A\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m:\n\u001b[1;32m    695\u001b[0m     \u001b[38;5;66;03m# If the input data has values outside the valid range (after\u001b[39;00m\n\u001b[1;32m    696\u001b[0m     \u001b[38;5;66;03m# normalisation), we issue a warning and then clip X to the bounds\u001b[39;00m\n\u001b[1;32m    697\u001b[0m     \u001b[38;5;66;03m# - otherwise casting wraps extreme values, hiding outliers and\u001b[39;00m\n\u001b[1;32m    698\u001b[0m     \u001b[38;5;66;03m# making reliable interpretation impossible.\u001b[39;00m\n\u001b[1;32m    699\u001b[0m     high \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m255\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39missubdtype(A\u001b[38;5;241m.\u001b[39mdtype, np\u001b[38;5;241m.\u001b[39minteger) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mTypeError\u001b[0m: Invalid shape (1, 320, 320) for image data"
     ]
    }
   ],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=2)\n",
    "im = ax[0].imshow(phase_np[0])\n",
    "fig.colorbar(im, ax=ax[0])\n",
    "im = ax[1].imshow(phase_adjust[0])\n",
    "fig.colorbar(im, ax=ax[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065c96d0-a308-4276-a6e5-cbb7d238f25d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c89a54-193c-4566-8282-bba5fa015341",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3be970-1dbf-47f1-a2f4-08a7f9829dcc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f080de8b-1a72-49f0-a719-87ff5aa77a4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256d1caf-40e8-46fe-bc27-85570341c9ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
